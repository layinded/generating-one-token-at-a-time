# Generating One Token at a Time

This notebook explores how Large Language Models (LLMs) generate text â€” **one token at a time** â€” using probability distributions and subword tokenization.

ğŸ§  **Concepts Covered**
- Subword tokenization
- Predictive token generation
- Using pretrained models with Hugging Face Transformers
- Visualizing logits and token probabilities

---

## ğŸ” Key Questions

### Q1: What is subword tokenization?

Subword tokenization is a method that splits words into smaller, more frequent units (e.g., "unbreakable" â†’ "un", "break", "able"). It reduces the size of the model's vocabulary while allowing it to understand rare or out-of-vocabulary words.

### Q2: Code Snippet Matching

| Code Snippet | Action |
|--------------|--------|
| `tokenizer` | Tokenize a piece of text |
| `AutoTokenizer.from_pretrained()` | Load a tokenizer |
| `AutoModelForCausalLM.from_pretrained()` | Load a pretrained model |
| `torch.nn.functional.softmax()` | Convert logits into probabilities |

---

## ğŸ›  Tools & Libraries

- Hugging Face Transformers
- PyTorch
- Tokenizer APIs
- Softmax & Logits

---

## ğŸ“ Included

- `Exercise2-generating-one-token-at-a-time.ipynb` (main notebook)
- Reflections in `notes/reflection.txt`

---

## ğŸ’¬ My Reflection

> â€œThis was the first time I fully understood how an LLM doesnâ€™t just *write*, it predicts â€” token by token, using math, probability, and context. Subword tokenization blew my mind a bit. You donâ€™t need to know every word if you can build them.â€

---

## ğŸ”— Learn More

Udacity Generative AI Nanodegree  
ğŸ“š [https://learn.udacity.com/nanodegrees/nd608-bmann-nextgen-challenge/](https://learn.udacity.com/nanodegrees/nd608-bmann-nextgen-challenge/)
